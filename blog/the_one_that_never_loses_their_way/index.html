<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><script async src="https://www.googletagmanager.com/gtag/js?id=G-4T0TFTB785"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-4T0TFTB785",{anonymize_ip:!1})}</script><script src=https://cdn.tailwindcss.com></script><meta name=title content="The One That Never Loses Their Way - Ivan"><meta name=description content="Eigenvalues and eigenvectors are often cited as crucial components in data science and machine learning, yet it is perplexing why they are not emphasized more in traditional data science curricula. Seeking answers, I pursued a second major in Mathematics and Statistics in my university.
The study dates back to 1858, when mathematician Arthur Cayley made a groundbreaking discovery with the Cayley-Hamilton theorem. This theorem states that if λ (the eigenvalue) is substituted with the original matrix, the characteristic polynomial for a matrix will remain true."><meta property="og:title" content="The One That Never Loses Their Way - Ivan"><meta property="og:description" content="Eigenvalues and eigenvectors are often cited as crucial components in data science and machine learning, yet it is perplexing why they are not emphasized more in traditional data science curricula. Seeking answers, I pursued a second major in Mathematics and Statistics in my university.
The study dates back to 1858, when mathematician Arthur Cayley made a groundbreaking discovery with the Cayley-Hamilton theorem. This theorem states that if λ (the eigenvalue) is substituted with the original matrix, the characteristic polynomial for a matrix will remain true."><meta property="og:url" content="https://orange-my-cat.github.io/blog/the_one_that_never_loses_their_way/"><meta property="og:type" content="website"><meta property="og:image" content="https://orange-my-cat.github.io/seo_image.png"><meta name=twitter:card content="summary"><meta name=twitter:title content="The One That Never Loses Their Way - Ivan"><meta name=twitter:description content="Eigenvalues and eigenvectors are often cited as crucial components in data science and machine learning, yet it is perplexing why they are not emphasized more in traditional data science curricula. Seeking answers, I pursued a second major in Mathematics and Statistics in my university.
The study dates back to 1858, when mathematician Arthur Cayley made a groundbreaking discovery with the Cayley-Hamilton theorem. This theorem states that if λ (the eigenvalue) is substituted with the original matrix, the characteristic polynomial for a matrix will remain true."><meta name=twitter:image content="https://orange-my-cat.github.io/seo_image.png"><link rel=canonical href=https://orange-my-cat.github.io/blog/the_one_that_never_loses_their_way/><link rel=icon href=/favicon.png><link rel=apple-touch-icon-precomposed sizes=144x144 href=/favicon.png><link rel=apple-touch-icon-precomposed sizes=114x114 href=/favicon.png><link rel=apple-touch-icon-precomposed sizes=72x72 href=/favicon.png><link rel=apple-touch-icon-precomposed href=/favicon.png><title>The One That Never Loses Their Way - Ivan</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Gruppo&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@200&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Quicksand&display=swap" rel=stylesheet><link href="https://fonts.googleapis.com/css2?family=Press+Start+2P&display=swap" rel=stylesheet><link rel=stylesheet href=/style.css></head><body class="bg-neutral-900 text-slate-300"><nav class=select-none><div class="flex justify-between lg:fixed top-0 left-0 right-0 z-0 text-slate-300"><div onclick='location.href="/"' class="lg:-rotate-90 font-semibold tracking-wider ml-2 text-4xl lg:mt-24 lg:ml-8 lg:text-8xl hover:text-red-600 cursor-pointer transition-colors duration-300" style=font-family:gruppo,cursive>IVAN</div><div class="-mt-0.5 lg:mt-8 lg:mr-8"><div id=openMenu onclick=openMenu() class="hover:text-red-600 cursor-pointer transition-colors duration-300"><svg class="h-12 w-12 lg:h-32 lg:w-32" xmlns="http://www.w3.org/2000/svg" fill="currentcolor" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M2.5 12a.5.5.0 01.5-.5h10a.5.5.0 010 1H3a.5.5.0 01-.5-.5zm0-4a.5.5.0 01.5-.5h10a.5.5.0 010 1H3A.5.5.0 012.5 8zm0-4a.5.5.0 01.5-.5h10a.5.5.0 010 1H3A.5.5.0 012.5 4z"/></svg></div></div></div><div id=menu class="flex fixed top-full w-full h-full bg-red-900 transition-transform duration-200 z-20"><div class="flex fixed w-full h-full font-semibold tracking-wider text-4xl lg:text-8xl" style=font-family:gruppo,cursive><div class="grid grid-flow-row auto-rows-max w-full content-center"><div class="text-center cursor-pointer text-white hover:text-red-400 transition-colors duration-300" onclick='location.href="/"'>HOME</div><div class="text-center cursor-pointer text-white hover:text-red-400 transition-colors duration-300" onclick='location.href="/blog"'>BLOG</div><div class="text-center cursor-pointer text-white hover:text-red-400 transition-colors duration-300" onclick='location.href="/projects"'>PROJECTS</div><div class="grid grid-flow-col auto-cols-max gap-x-8 lg:gap-x-16 w-full justify-center pt-8 lg:pt-12"><div class="cursor-pointer text-white hover:text-red-400 transition-colors duration-300" onclick='location.href="https://github.com/orange-my-cat"'><svg class="h-4 w-4 lg:h-12 lg:w-12" xmlns="http://www.w3.org/2000/svg" fill="currentcolor" viewBox="0 0 16 16"><path d="M8 0C3.58.0.0 3.58.0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38.0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95.0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12.0.0.67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15.0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48.0 1.07-.01 1.93-.01 2.2.0.21.15.46.55.38A8.012 8.012.0 0016 8c0-4.42-3.58-8-8-8z"/></svg></div><div class="cursor-pointer text-white hover:text-red-400 transition-colors duration-300" onclick='location.href="https://www.linkedin.com/in/ivanpeh/"'><svg class="h-4 w-4 lg:h-12 lg:w-12" xmlns="http://www.w3.org/2000/svg" fill="currentcolor" viewBox="0 0 16 16"><path d="M0 1.146C0 .513.526.0 1.175.0h13.65C15.474.0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487.0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837.0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822.0-1.359.54-1.359 1.248.0.694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869.0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274.0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54.0 01.016-.025V6.169h-2.4c.03.678.0 7.225.0 7.225h2.4z"/></svg></div></div></div></div><div id=closeMenu onclick=closeMenu() class="absolute top-0 right-0 mt-2 mr-2 lg:mt-8 lg:mr-8 text-white hover:text-black cursor-pointer transition-colors duration-300"><svg class="h-12 w-12 lg:h-28 lg:w-28" xmlns="http://www.w3.org/2000/svg" fill="currentcolor" viewBox="0 0 16 16"><path d="M2.146 2.854a.5.5.0 11.708-.708L8 7.293l5.146-5.147a.5.5.0 01.708.708L8.707 8l5.147 5.146a.5.5.0 01-.708.708L8 8.707l-5.146 5.147a.5.5.0 01-.708-.708L7.293 8 2.146 2.854z"/></svg></div></div></nav><script>function openMenu(){menu=document.getElementById("menu"),menu.classList.add("-translate-y-full")}function closeMenu(){menu=document.getElementById("menu"),menu.classList.remove("-translate-y-full")}</script><div class="my-8 mx-4 lg:ml-56 lg:mr-40 z-10 relative" style=font-family:quicksand,sans-serif><h1 class="mb-8 lg:mr-0 text-3xl lg:text-6xl font-bold text-center">The One That Never Loses Their Way</h1><div class="text-base lg:text-xl">Posted on 02-03-2023</div><div class="mb-4 lg:mb-6 text-base lg:text-xl"><a href=/tags/knowledge class="hover:text-red-600 cursor-pointer transition-colors duration-300">#knowledge</a>
<a href=/tags/mathematics class="hover:text-red-600 cursor-pointer transition-colors duration-300">#mathematics</a></div><article class=lg:text-justify><p><a href=https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors>Eigenvalues and eigenvectors</a> are often cited as crucial components in data science and machine learning, yet it is perplexing why they are not emphasized more in traditional data science curricula. Seeking answers, I pursued a second major in Mathematics and Statistics in my university.</p><p>The study dates back to <a href=https://www.jstor.org/stable/108649>1858</a>, when mathematician Arthur Cayley made a groundbreaking discovery with the Cayley-Hamilton theorem. This theorem states that if <em><code>λ</code></em> (the eigenvalue) is substituted with the original matrix, the characteristic polynomial for a matrix will remain true. Example below:</p><pre tabindex=0><code>A = [1, 2]
    [2, 1]

which makes the characteristic polynomial be:

det(A-λI) = λ^2 - 2λ - 3

we obviously find λ = -1 and 3 when det(A-λI) = 0

replacing λ with A, 
we find that A^2 - 2A - 3I = 0
</code></pre><p></p><p>Finding eigenvalues and eigenvectors are quite straightforward. Eigenvalues are found by finding <em><code>λ</code></em> just like in the equation above while eigenvectors have to satisfy <em><code>Av = λv</code></em>. As in the case above:</p><pre tabindex=0><code>[1, 2][x] = [λx]
[2, 1][y]   [λy]

when λ = -1:

x + 2y = -x and 2x + y = -y
we find that x = -y
thus, v = [x, -x] 

when λ = 3:
x + 2y = 3x and 2x + y = 3y
we find that x = y
thus, v = [x, x]
</code></pre><p></p><p>But how is it so important? And what practical applications can be derived from <em><code>λ</code></em> and <em><code>v</code></em>? For starters, we need to understand what is it that we have. We can understand vectors as points in a coordinate system, while multiplying matrices with vectors transforms the vectors. In a 2D plane, we would have a vector [x, y]. In which we can multiply the vector with a 2x2 matrix to transform it. A common matrix would be a scaling matrix as shown below.</p><pre tabindex=0><code>the scaling matrix in a 2D plane is defined as:

[k, 0]
[0, k]

where k = scale along the x-axis and y-axis
</code></pre><p></p><p>Knowing that, we can understand the eigenvector as the vector that only changes in scale when transformed by a matrix, while the eigenvalue is the scale which the vector changed. As in the image below, the vectors are scaled in both the x-axis and y-axis. Notice that all vectors in the image is still in its original direction but just scaled by a constant amount. This should mean that any vector can be an eigenvector for the scaling matrix right?</p><p><img src=/blogs/matrix_scaling.png alt=matrix_scaling.png></p><pre tabindex=0><code>the characteristic polynomial is defined as:

λ^2 - λk^2 + k^2 = 0

as an example, we let k = 2 to scale the image by 2 times

the characteristic polynomial is then:
λ^2 - 4λ + 4 = 0

we can derive that λ = 2

then we can calculate the eigenvectors using the eigenvalue above

[2, 0][x] = [2x]
[0, 2][y]   [2y]

we get that 2x = 2x and 2y = 2y
therefore our eigenvector would be [x, y]
</code></pre><p></p><p>This implies that any sets of vectors multiplied by a scaling matrix would just have every vector scaled by the same amount without any change in direction. Makes sense right? Other than the simple scaling matrix, there also exists rotation, translation, reflection, and shearing matrices, which of course is not as easy as the scaling matrix. With this, we can see that eigenvalues and eigenvectors are a big thing in computer graphics.</p><p>In data science however, the vectors provided is not as simple as being coordinates in a plane, they may be entire rows of data or entire columns of data. Here however, we may not need to perform rotations or reflections to the data. Instead, we can use eigenvalues and eigenvectors to perform Principal Component Analysis (PCA). This allows us to significantly reduce the number of variables required for analysis. In PCA, instead of using a transformation matrix, we first need to find the eigenvalues and eigenvectors of its covariance matrix. Then we can easily transform all data points using a single matrix. To learn more about PCA, I recommend watching this <a href="https://www.youtube.com/watch?v=FgakZw6K1QQ">video</a>!</p><p><img src=/blogs/eigenvector.png alt=eigenvector.png></p></article></div><footer class="mx-4 lg:ml-56 lg:mr-40 relative"><br><hr class=border-gray-300><br><div class=text-center style=font-family:quicksand,sans-serif><p><a href=https://github.com/orange-my-cat class="underline hover:text-red-600 transition-colors duration-300">github</a>
<a href=https://www.linkedin.com/in/ivanpeh/ class="underline hover:text-red-600 transition-colors duration-300">linkedin</a></p><br><p>site powered by spite, cats and
<a href=https://gohugo.io/ class="underline hover:text-red-600 transition-colors duration-300">hugo</a></p></div><br></footer></body></html>