<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>mathematics on Ivan</title><link>https://orange-my-cat.github.io/tags/mathematics/</link><description>Recent content in mathematics on Ivan</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 02 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://orange-my-cat.github.io/tags/mathematics/index.xml" rel="self" type="application/rss+xml"/><item><title>The One That Never Loses Their Way</title><link>https://orange-my-cat.github.io/blog/the_one_that_never_loses_their_way/</link><pubDate>Thu, 02 Mar 2023 00:00:00 +0000</pubDate><guid>https://orange-my-cat.github.io/blog/the_one_that_never_loses_their_way/</guid><description>Eigenvalues and eigenvectors are often cited as crucial components in data science and machine learning, yet it is perplexing why they are not emphasized more in traditional data science curricula. Seeking answers, I pursued a second major in Mathematics and Statistics in my university.
The study dates back to 1858, when mathematician Arthur Cayley made a groundbreaking discovery with the Cayley-Hamilton theorem. This theorem states that if Î» (the eigenvalue) is substituted with the original matrix, the characteristic polynomial for a matrix will remain true.</description></item></channel></rss>